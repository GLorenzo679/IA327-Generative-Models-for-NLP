{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863ccaa-86c7-47fd-84f3-89ff94ecfc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell A\n",
    "# load data\n",
    "import os\n",
    "github_path = \"https://raw.githubusercontent.com/najoungkim/COGS/main/data\"\n",
    "data = {}\n",
    "for name in ['train', 'dev', 'test', 'gen']:\n",
    "    url = os.path.join(github_path, '{}.tsv'.format(name))\n",
    "    filepath = '{}.tsv'.format(name)\n",
    "    if not os.path.isfile(filepath):\n",
    "        os.system('wget {}'.format(url))\n",
    "    assert os.path.isfile(filepath)\n",
    "    lines = open(filepath, 'r')\n",
    "    lines = map(lambda x: x.strip(), lines)\n",
    "    lines = map(lambda x: x.split('\\t')[0], lines)\n",
    "    data[name] = list(lines)\n",
    "\n",
    "# look at the data\n",
    "for name in ['train', 'dev', 'test', 'gen']:\n",
    "    print('=== {} ({} sentences)==='.format(name.upper(), len(data[name])))\n",
    "    print('\\n'.join(data[name][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0862c-a85f-4f7d-8cdb-afebbebe7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell B\n",
    "# Tokenization\n",
    "# The final period is separated with a white space from the final word. This is a basic form of tokenization.\n",
    "# The same is done as a pre-processing step with all punctuation (comma, question mark...).\n",
    "# Turn sentences into lists of tokens: (here, token = word or punctuation mark; more generally, any sequence of ascii considered a unit)\n",
    "for name in data:\n",
    "    data[name] = list(map(lambda x: x.split(), data[name]))\n",
    "# Normalization\n",
    "# Here, uppercase vs lowercase is not so useful, and it just increases the size of the vocabulary (see below).\n",
    "# So, let's turn all words into lowercase. Other possible normalization includes stemming.\n",
    "# After this last stage of pre-processing, we have tokens.\n",
    "tokens = {}\n",
    "for name in data:\n",
    "    tokens[name] = list(map(lambda x: [y.lower() for y in x], data[name]))\n",
    "# Finally, add beginning-of-sentence and end-of-sentence tokens\n",
    "for name in tokens:\n",
    "    tokens[name] = list(map(lambda x: ['<bos>'] + x + ['<eos>'], tokens[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898ec0e-d384-4c7c-b446-99999288b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell C\n",
    "# Vocabulary\n",
    "# Let's set up the alphabet (noted V during the lecture), usually called vocabulary to avoid confusion\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "    def add(self, word):\n",
    "        self.vocab.add(word)\n",
    "    def process(self, sentence):\n",
    "        assert isinstance(sentence, list) or isinstance(sentence, tuple)\n",
    "        assert all(isinstance(x, str) for x in sentence)\n",
    "        self.vocab.update(set(sentence))\n",
    "    def size(self):\n",
    "        return len(self.vocab)\n",
    "# Create the vocabulary from the training data\n",
    "vocabulary = Vocabulary()\n",
    "for sentence in tokens['train']:\n",
    "    vocabulary.process(sentence)\n",
    "# Q1: What is the size of the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129596d-5022-4cec-9abc-7bf758e19133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell D\n",
    "# n-gram model\n",
    "import math\n",
    "\n",
    "class Probability:\n",
    "    # special class to store small probabilities\n",
    "    def __init__(self, p, log_scale = False):\n",
    "        if log_scale:\n",
    "            assert p >= 0, p\n",
    "            self.value = p\n",
    "        else:\n",
    "            assert p <= 1, p\n",
    "            assert p >= 0, p\n",
    "            if p == 0:\n",
    "                self.value = float('inf')\n",
    "            else:\n",
    "                self.value = -math.log(p)\n",
    "\n",
    "    def get_p(self):\n",
    "        p = math.exp(-self.value)\n",
    "        assert p >= 0, p\n",
    "        assert p <= 1, p\n",
    "        return p\n",
    "\n",
    "    def multiply(self, p):\n",
    "        assert isinstance(p, Probability)\n",
    "        self.value += p.value\n",
    "        \n",
    "class NgramCounter:\n",
    "    def __init__(self, n, smoothing=0):\n",
    "        assert isinstance(n, int)\n",
    "        assert n > 0\n",
    "        assert isinstance(smoothing, int)\n",
    "        assert smoothing >= 0\n",
    "        self.n = n # the length of ngrams that we store\n",
    "        self.smoothing = smoothing\n",
    "        self.ngrams = {}\n",
    "        self.vocab = Vocabulary()\n",
    "\n",
    "    def count(self, ngram):\n",
    "        assert isinstance(ngram, tuple)\n",
    "        assert len(ngram) <= self.n\n",
    "        self.ngrams.setdefault(ngram, 0)\n",
    "        self.ngrams[ngram] += 1\n",
    "        self.vocab.process(ngram)\n",
    "\n",
    "    def shorten_history(self, history, max_length=None):\n",
    "        if max_length is None:\n",
    "            max_length = self.n-1\n",
    "        if max_length == 0:\n",
    "            return []\n",
    "        if len(history) > max_length:\n",
    "            history = history[-max_length:]\n",
    "        return history\n",
    "        \n",
    "    def process(self, tokens):\n",
    "        assert isinstance(tokens, list)\n",
    "        assert all(isinstance(x, str) for x in tokens)\n",
    "        for tt in range(len(tokens)):\n",
    "            for ll in range(1,self.n+1): # we need to count all lengths of ngrams\n",
    "                if tt+ll > len(tokens):\n",
    "                    break\n",
    "                ngram = tokens[tt:tt+ll]\n",
    "                self.count(tuple(ngram))\n",
    "\n",
    "    def get_count(self, ngram, history=False):\n",
    "        if isinstance(ngram, str):\n",
    "            count = self.ngrams.get((ngram,), 0)\n",
    "        else:\n",
    "            count = self.ngrams.get(tuple(ngram), 0)\n",
    "        if history:\n",
    "            count += self.smoothing*self.vocab.size()\n",
    "        else:\n",
    "            count += self.smoothing\n",
    "        return count\n",
    "\n",
    "    def get_unigram_count(self):\n",
    "        output = 0\n",
    "        for ngram, count in self.ngrams.items():\n",
    "            if len(ngram) == 1:\n",
    "                output += count\n",
    "        output += self.smoothing*self.vocab.size()\n",
    "        return output\n",
    "    \n",
    "    def conditional_probability(self, token, history):\n",
    "        # return p(token | history)\n",
    "        assert isinstance(token, str)\n",
    "        assert isinstance(history, list)\n",
    "        assert all(isinstance(x, str) for x in history)\n",
    "        history = self.shorten_history(history)\n",
    "        ngram = tuple(history) + (token,)\n",
    "        count = self.get_count(ngram)\n",
    "        assert count is not None, ngram\n",
    "        if len(history) == 0:\n",
    "            Z = self.get_unigram_count()\n",
    "            assert Z > 0, Z\n",
    "            return Probability(count/Z)\n",
    "        else:\n",
    "            Z = self.get_count(history, history=True)\n",
    "            assert Z > 0, '{}: {}'.format(history, Z)\n",
    "            return Probability(count/Z)\n",
    "    \n",
    "    def probability(self, tokens):\n",
    "        logprob = self.compute_logprob(tokens) \n",
    "        return logprob.get_p()\n",
    "\n",
    "    def compute_logprob(self, tokens):\n",
    "        probs = [None for _ in tokens]\n",
    "        for tt in range(len(tokens)):\n",
    "            history = tokens[:tt]\n",
    "            cp = self.conditional_probability(tokens[tt], history)\n",
    "            probs[tt] = cp\n",
    "        assert all(x is not None for x in probs)\n",
    "        logprob = sum(x.value for x in probs)\n",
    "        logprob = Probability(logprob, log_scale = True)\n",
    "        return logprob\n",
    "\n",
    "    def perplexity(self, tokens):\n",
    "        logprob = self.compute_logprob(tokens)\n",
    "        perplexity = Probability(logprob.value/len(tokens), log_scale = True)\n",
    "        return perplexity.value\n",
    "\n",
    "    def get_top_ngrams(self, length=None, n=10):\n",
    "        ngrams = sorted(self.ngrams.items(), key=lambda x: (-x[1], x[0]))\n",
    "        if length is not None:\n",
    "            ngrams = list(filter(lambda x: len(x[0]) == length, ngrams))\n",
    "        return ngrams[:n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ae972-3dee-409f-8931-ae39bb2d39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell E\n",
    "# build ngram model\n",
    "def fit(model, data):\n",
    "    for sentence in data:\n",
    "        model.process(sentence)\n",
    "ngram_model = NgramCounter(25)\n",
    "fit(ngram_model, tokens['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c47fa-6d20-4a2e-9ea1-ffc337a7a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell F\n",
    "# Q2: What do you expect the following probabilities to be? Why?\n",
    "print(ngram_model.conditional_probability('<eos>', ['.']).get_p())\n",
    "print(ngram_model.conditional_probability('<eos>', ['<bos>']).get_p())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5433f79-1ce3-4c85-bf43-ccf6d99e5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell G\n",
    "# Look at the top ngrams\n",
    "for n in range(1,ngram_model.n+1):\n",
    "    print('=== {} ==='.format(n))\n",
    "    top_ngrams = ngram_model.get_top_ngrams(length=n)\n",
    "    for x in top_ngrams:\n",
    "        print('{}: {}'.format(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffada06-85eb-4c72-a82a-242b29f4fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell H\n",
    "# Zipf's law https://en.wikipedia.org/wiki/Zipf's_law\n",
    "# Q3: Run this code to produce a plot. What does this plot show? What is on the x-axis, what is on the y-axis?\n",
    "# Q4: According to this plot, is the ngram assumption justified? If you don't remember what the ngram assumption is, ask Nils.\n",
    "%matplotlib ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_ngram_frequencies(n):\n",
    "    frequencies = filter(lambda x: len(x[0]) == n, ngram_model.ngrams.items())\n",
    "    frequencies = map(lambda x: x[1], frequencies)\n",
    "    y = sorted(frequencies, reverse = True)\n",
    "    x = np.arange(len(y))\n",
    "    if len(x) == 0:\n",
    "        return None, None\n",
    "    else:\n",
    "        return x, y\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "for n in range(1,11):\n",
    "    x, y = get_ngram_frequencies(n)\n",
    "    if x is None:\n",
    "        continue\n",
    "    ax.plot(x, y, label=n)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcdebc7-b6de-4b59-bc1a-b73e17637680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell I\n",
    "# Perplexity\n",
    "from tqdm import tqdm\n",
    "def dataset_perplexity(model, data, progress=False, details=False):\n",
    "    output = 0\n",
    "    samples = [] if details else None\n",
    "    it = tqdm(data) if progress else data\n",
    "    for sentence in it:\n",
    "        perplexity_ = model.perplexity(sentence)\n",
    "        output += perplexity_\n",
    "        if samples is not None:\n",
    "            samples.append((sentence, perplexity_))\n",
    "    output /= len(data)\n",
    "    if details:\n",
    "        return output, sorted(samples, key=lambda x: x[1])\n",
    "    else:\n",
    "        return output\n",
    "# Q5: Why does this throw an error?\n",
    "perplexity = dataset_perplexity(ngram_model, tokens['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc2e31-8bef-40b6-af4a-233aeb6a9d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell J\n",
    "# Try again, but with \"smoothing = 1\"\n",
    "# \"smoothing\" adds counts to unknown ngrams. No probability is equal to 0 anymore,\n",
    "# but we give probability mass away that could be used for known ngrams.\n",
    "ngram_model = NgramCounter(25, smoothing=1)\n",
    "fit(ngram_model, tokens['train'])\n",
    "perplexity = dataset_perplexity(ngram_model, tokens['dev'], progress=True)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a050a61-f214-43d4-97c0-3b9c4db10f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell K\n",
    "# small grid search: find the best possible length for ngrams, and the best possible smoothing value\n",
    "# while this runs, get up and stretch your legs\n",
    "import itertools\n",
    "n_values = [1,2,3,4]\n",
    "smoothing_values = [1,2,4]\n",
    "params = list(itertools.product(*[n_values, smoothing_values]))\n",
    "for param in itertools.product(*[n_values, smoothing_values]):\n",
    "    n, smoothing = param\n",
    "    ngram_model = NgramCounter(n, smoothing=smoothing)\n",
    "    fit(ngram_model, tokens['train'])\n",
    "    perplexity = dataset_perplexity(ngram_model, tokens['dev'])\n",
    "    print('n={}, smoothing={}, perplexity={}'.format(n, smoothing, perplexity))\n",
    "# Q6: Why doesn't the dev perplexity keep decreasing as n increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c869fb-7952-4f87-bdf9-2987c5ae4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell L\n",
    "# Q7: What are the best values for n and smoothing?\n",
    "best_n = # TODO\n",
    "best_smoothing = # TODO\n",
    "# Q8: How many parameters does the best ngram model have? Explain how you compute this quantity.\n",
    "best_ngram_model = NgramCounter(best_n, smoothing=best_smoothing)\n",
    "fit(best_ngram_model, tokens['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a62cd5-e667-4983-b9e1-d5c936f914e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell M\n",
    "# Build the best model on train and dev using the parameters for n and smoothing found in cell L.\n",
    "# Then compute its perplexity on test and gen.\n",
    "# Q9: What do you notice?\n",
    "best_n = # TODO\n",
    "best_smoothing = # TODO\n",
    "ngram_model = NgramCounter(best_n, smoothing=best_smoothing)\n",
    "fit(ngram_model, tokens['train']+tokens['dev'])\n",
    "for split in ['train', 'dev', 'test', 'gen']:\n",
    "    perplexity = dataset_perplexity(ngram_model, tokens[split], progress=True)\n",
    "    print('{}: {}'.format(split, perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b855a5-11f8-40ba-b6a6-3be869cbb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell N\n",
    "# RNN LM\n",
    "# Q10: How does this RNN LM deal with words outside of its vocabulary?\n",
    "import torch\n",
    "from torch import nn\n",
    "import itertools\n",
    "\n",
    "# redefine this helper class\n",
    "class Vocabulary:\n",
    "    def __init__(self, words=set()):\n",
    "        self.vocab = set(words)\n",
    "        self.unk = '<unk>'\n",
    "        self.bos = '<bos>'\n",
    "        self.eos = '<eos>'\n",
    "        self.vocab.add(self.unk)\n",
    "        self.vocab.add(self.bos)\n",
    "        self.vocab.add(self.eos)\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        \n",
    "    def add(self, word):\n",
    "        if word not in self.vocab:\n",
    "            self.vocab.append(word)\n",
    "        \n",
    "    def process(self, sentence):\n",
    "        assert isinstance(sentence, list) or isinstance(sentence, tuple)\n",
    "        assert all(isinstance(x, str) for x in sentence)\n",
    "        for word in sentence:\n",
    "            self.add(word)\n",
    "        \n",
    "    def size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def word_to_int(self, word):\n",
    "        assert isinstance(word, str)\n",
    "        if word in self.vocab:\n",
    "            key = word\n",
    "        else:\n",
    "            key = self.unk\n",
    "        return self.vocab.index(key)\n",
    "\n",
    "    def sentence_to_int(self, sentence):\n",
    "        output = [None for _ in sentence]\n",
    "        for ii, token in enumerate(sentence):\n",
    "            output[ii] = self.word_to_int(token)\n",
    "        return output\n",
    "        \n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab=None, n_layers=2, n_units=512):\n",
    "        super(RNNLM, self).__init__()\n",
    "        assert isinstance(n_layers, int)\n",
    "        assert n_layers > 0\n",
    "        self.n_layers = n_layers\n",
    "        assert isinstance(n_units, int)\n",
    "        assert n_units > 0\n",
    "        self.n_units = n_units\n",
    "        assert isinstance(vocab, Vocabulary)\n",
    "        self.vocab = vocab\n",
    "        self.padding = self.vocab.size()\n",
    "        # model parameters\n",
    "        # dense word embeddings\n",
    "        self.embeddings = nn.Embedding(self.vocab.size()+1, self.n_units, padding_idx=self.padding)\n",
    "        # recurrent neural network\n",
    "        self.rnn = nn.LSTM(self.n_units, self.n_units, num_layers=self.n_layers, batch_first=True)\n",
    "        # final projection layer\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.n_units, self.vocab.size()),\n",
    "            nn.LogSoftmax(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        assert isinstance(batch, list), batch\n",
    "        assert all(isinstance(x, list) for x in batch), batch\n",
    "        assert all(all(isinstance(x, str) for x in sentence) for sentence in batch)\n",
    "        assert all(x[0] == self.vocab.bos for x in batch)\n",
    "        assert all(x[-1] == self.vocab.eos for x in batch)        \n",
    "        # turn into indices\n",
    "        indices = [self.vocab.sentence_to_int(x) for x in batch]\n",
    "        # pad\n",
    "        max_len = max(len(x) for x in indices)\n",
    "        indices = [x if len(x) == max_len else x+[self.padding]*(max_len-len(x)) for x in indices]\n",
    "        # turn into tensor\n",
    "        indices = torch.tensor(indices)\n",
    "        # feed through model\n",
    "        word_embeddings = self.embeddings(indices)\n",
    "        rnn_outputs, _ = self.rnn(word_embeddings)\n",
    "        logits = self.projection(rnn_outputs)\n",
    "        return logits, indices\n",
    "\n",
    "    def perplexity(self, sentence):\n",
    "        assert isinstance(sentence, list), sentence\n",
    "        assert all(isinstance(x, str) for x in sentence)\n",
    "        assert sentence[0] == self.vocab.bos, sentence\n",
    "        assert sentence[-1] == self.vocab.eos, sentence\n",
    "        logits, indices = self.forward([sentence])\n",
    "        logits = logits[0,:-1,:]\n",
    "        indices = indices[0,1:]\n",
    "        # there is probably a better way for the following\n",
    "        perplexity = 0\n",
    "        for tt, index in enumerate(indices.numpy().tolist()):\n",
    "            perplexity += logits[tt,index].item()\n",
    "        perplexity = -perplexity/len(sentence)\n",
    "        return perplexity\n",
    "\n",
    "    def batch_perplexity(self, batch):\n",
    "        assert isinstance(batch, list), batch\n",
    "        assert all(isinstance(x, list) for x in batch), batch\n",
    "        assert len(batch) > 0\n",
    "        perplexities = [ self.perplexity(sentence) for sentence in batch ]\n",
    "        return sum(perplexities)/len(perplexities)\n",
    "            \n",
    "    def number_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, data, batch_size=128):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        assert isinstance(batch_size, int)\n",
    "        assert batch_size>0\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_function = nn.CrossEntropyLoss(ignore_index=self.model.padding, size_average=True)\n",
    "\n",
    "    def step(self, split, position, grad=False):\n",
    "        start = position\n",
    "        end = min(start + self.batch_size, len(self.data[split]))\n",
    "        batch = self.data[split][start:end]\n",
    "        logits, indices = self.model.forward(batch)\n",
    "        targets = indices[:,1:]\n",
    "        logits = logits[:,:-1,:]\n",
    "        assert logits.size(0) == targets.size(0)\n",
    "        assert logits.size(1) == targets.size(1)\n",
    "        logits = torch.reshape(logits, (-1, self.model.vocab.size()))\n",
    "        targets = torch.reshape(targets, (-1,))\n",
    "        loss = self.loss_function(logits, targets)\n",
    "        if grad:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), 5)\n",
    "            self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def train(self, num_epochs, verbose=False):\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0\n",
    "            num_steps = 0\n",
    "            it = tqdm(range(0, len(self.data['train']), self.batch_size)) if verbose \\\n",
    "                else range(0, len(self.data['train']), self.batch_size)\n",
    "            for position in it:\n",
    "                loss_ = self.step('train', position, grad=True)\n",
    "                train_loss += loss_.item()\n",
    "                num_steps += 1\n",
    "            train_loss /= num_steps\n",
    "            dev_loss = 0\n",
    "            num_steps = 0\n",
    "            it = tqdm(range(0, len(self.data['dev']), self.batch_size)) if verbose \\\n",
    "                else range(0, len(self.data['dev']), self.batch_size)\n",
    "            for position in it:\n",
    "                loss_ = self.step('dev', position, grad=False)\n",
    "                dev_loss += loss_\n",
    "                num_steps += 1\n",
    "            dev_loss /= num_steps\n",
    "            print('epoch {}/{}; train loss {}; dev loss {}'.format(epoch+1, num_epochs, train_loss, dev_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234259c-716f-447c-9d0e-087ddd303aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell O\n",
    "# Train RNN LM\n",
    "vocabulary = Vocabulary()\n",
    "for sentence in tokens['train']:\n",
    "    vocabulary.process(sentence)\n",
    "rnn_lm = RNNLM(vocab=vocabulary, n_layers=1, n_units=7)\n",
    "print(rnn_lm)\n",
    "print('number of parameters: {}'.format(rnn_lm.number_parameters()))\n",
    "trainer = Trainer(rnn_lm, tokens)\n",
    "print('get up and stretch your legs while this runs')\n",
    "trainer.train(100)\n",
    "# Q11: How do the number of parameters of this model compare to the number of parameters of the best ngram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f5242-b956-4b8b-9726-bffc0810c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell P\n",
    "# Q12: How does the following compare to the best ngram model?\n",
    "for split in ['train', 'dev', 'test', 'gen']:\n",
    "    perplexity = dataset_perplexity(rnn_lm, tokens[split], progress=True)\n",
    "    print('{}: {}'.format(split, perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766cbea-af09-4fa8-a11e-1c9e8047e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell Q\n",
    "# Note that we underexploited at least 3 things:\n",
    "# 1. The training time. The dev loss keeps decreasing, even after 200 epochs of training.\n",
    "# See this by running trainer.train(100)\n",
    "# 2. The RNN size. This RNN is *tiny* compared to what you'd use in practice. Here we just made it comparable to the best ngram model.\n",
    "# 3. The data. Once we know at what epoch our RNN converges, we can train on train+dev, using the optimal epoch number as our stopping criterion.\n",
    "# Q13: What else could we have done to further push the dev loss down?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc6865e-7e2c-466a-9c9e-eb68b8d46a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell R\n",
    "# Train larger RNN LM\n",
    "vocabulary = Vocabulary()\n",
    "for sentence in tokens['train']:\n",
    "    vocabulary.process(sentence)\n",
    "rnn_lm_large = RNNLM(vocab=vocabulary, n_layers=2, n_units=512)\n",
    "print(rnn_lm_large)\n",
    "print('number of parameters: {}'.format(rnn_lm_large.number_parameters()))\n",
    "trainer_large = Trainer(rnn_lm_large, tokens)\n",
    "print('get up and stretch your legs while this runs')\n",
    "trainer_large.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d36f9-ef30-42bd-8a52-2ecb292e005a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell S\n",
    "# Q14: How does the following compare to the other models?\n",
    "for split in ['train', 'dev', 'test', 'gen']:\n",
    "    perplexity = dataset_perplexity(rnn_lm_large, tokens[split], progress=True)\n",
    "    print('{}: {}'.format(split, perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d8e78-cc7b-4523-ae19-ce446affa1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell T\n",
    "# Look at some high and low perplexity samples\n",
    "# Look at overall distribution of perplexity\n",
    "# Q15: What do you notice? Is there a phrase that describes this mismatch between distributions?\n",
    "fig, ax = plt.subplots(1,3)\n",
    "for ii, (name, model) in enumerate([('ngram', ngram_model), ('rnn', rnn_lm), ('rnn large', rnn_lm_large)]):\n",
    "    for split in ['gen', 'test']:\n",
    "        perplexity, samples = dataset_perplexity(model, tokens[split], progress=True, details=True)\n",
    "        print('{} on {}:'.format(name, split))\n",
    "        print('  perplexity: {}'.format(perplexity))\n",
    "        print('  low perplexity:')\n",
    "        low_p = ('    '+' '.join(x[0]) + ' -- {}'.format(x[1]) for x in samples[:5])\n",
    "        print('\\n'.join(low_p))\n",
    "        print('  high perplexity:')\n",
    "        high_p = ('    '+' '.join(x[0]) + ' -- {}'.format(x[1]) for x in samples[-5:])\n",
    "        print('\\n'.join(high_p))\n",
    "        y = sorted((x[1] for x in samples), reverse=True)\n",
    "        assert all(z>0 for z in y)\n",
    "        x = np.arange(len(y))\n",
    "        #ax.plot(x, y, label='{}-{}'.format(name,split))\n",
    "        ax[ii].hist(y, bins=100, density=True, label='{}-{}'.format(name,split))\n",
    "for ii,_ in enumerate(ax):\n",
    "    ax[ii].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
