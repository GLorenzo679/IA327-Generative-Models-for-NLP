{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccfb45e4-daa5-4616-84b0-6f129ebe79fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/ids/glorenzo-23/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell A\n",
    "# dowload data: fragment of the Penn treebank\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import nltk.tree.tree as ntree\n",
    "import torch\n",
    "\n",
    "nltk.download(\"treebank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796524b2-2fae-4e7a-b926-815522ddc80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 3169 samples\n",
      "dev: 353 samples\n",
      "test: 392 samples\n",
      "max parser steps at inference time: 189\n"
     ]
    }
   ],
   "source": [
    "# Cell B\n",
    "def depth_first_traversal(tree):\n",
    "    if isinstance(tree, ntree.Tree):\n",
    "        label = \"{}\".format(tree.label())\n",
    "        output = [label]\n",
    "        for child_traversal, isterminal in [depth_first_traversal(t) for t in tree]:\n",
    "            if isterminal:\n",
    "                output.append(child_traversal)\n",
    "            else:\n",
    "                output.extend(child_traversal)\n",
    "        return (output + [\"REDUCE\"], False)\n",
    "    else:\n",
    "        assert isinstance(tree, str)\n",
    "        return \"SHIFT\", True\n",
    "\n",
    "\n",
    "data = []  # items\n",
    "for fileid in treebank.fileids():\n",
    "    gold_trees = treebank.parsed_sents(fileid)\n",
    "    for tree in gold_trees:\n",
    "        sentence = list(tree.leaves())\n",
    "        action_sequence, _ = depth_first_traversal(tree)\n",
    "        num_shifts = len(list(filter(lambda x: x == \"SHIFT\", action_sequence)))\n",
    "        assert num_shifts == len(sentence), \"{}: {}\".format(fileid, sentence)\n",
    "        data.append((sentence, action_sequence, tree))\n",
    "\n",
    "# split train-dev-test\n",
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(0)\n",
    "data.sort()\n",
    "random.shuffle(data)\n",
    "\n",
    "\n",
    "def split(list, fraction):\n",
    "    index = int(math.ceil(len(list) * fraction))\n",
    "    return list[:index], list[index:]\n",
    "\n",
    "\n",
    "# train, dev, test = data[:100], data[100:200], data[200:300]\n",
    "test, train = split(data, 0.1)  # 10% training data\n",
    "dev, train = split(train, 0.1)  # 10% of train is for dev\n",
    "data = {\"train\": train, \"dev\": dev, \"test\": test}\n",
    "action_lengths = sorted([len(x[1]) for x in train])\n",
    "MAX_STEPS = action_lengths[int(0.9 * len(action_lengths))]\n",
    "for split, d in data.items():\n",
    "    print(\"{}: {} samples\".format(split, len(d)))\n",
    "print(\"max parser steps at inference time: {}\".format(MAX_STEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204952a4-b819-4c60-8ea9-d34aee8bd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell C\n",
    "# shift-reduce parser\n",
    "class ShiftReduceParser:\n",
    "    def __init__(self, reduce=\"REDUCE\", shift=\"SHIFT\", no_op=\"NOOP\"):\n",
    "        self.reduce_kw = reduce\n",
    "        self.shift_kw = shift\n",
    "        self.noop_kw = no_op  # this is used to do nothing, for padding\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = []\n",
    "        self.stack = []\n",
    "\n",
    "    def initialize_with_sentence(self, sentence):\n",
    "        assert isinstance(sentence, list)\n",
    "        self.reset()\n",
    "        self.buffer = sentence[::-1]\n",
    "        assert len(self.stack) == 0\n",
    "\n",
    "    def shift(self):\n",
    "        if len(self.buffer) == 0:\n",
    "            return\n",
    "        word = self.buffer.pop()\n",
    "        self.stack.append((word, True))\n",
    "\n",
    "    def print_state(self):\n",
    "        stack_str = \"STACK: {}\".format(self.stack)\n",
    "        buffer_str = \"BUFFER: {}\".format(self.buffer)\n",
    "        return \"\\n\".join([stack_str, buffer_str])\n",
    "\n",
    "    def reduce(self):\n",
    "        # pop stack until an uncompleted nonterminal is encountered\n",
    "        completed_items = []\n",
    "        head = None\n",
    "        while True:\n",
    "            if not self.stack:\n",
    "                break\n",
    "            item = self.stack.pop()\n",
    "            if not item[1]:  # item is uncompleted\n",
    "                head = item  # so this will be the head in the reduce operation\n",
    "                break\n",
    "            completed_items.append(item)\n",
    "        # end while True:\n",
    "        if head is None:\n",
    "            # put stuff back on the stack! otherwise calling reduce twice could delete items from the stack\n",
    "            self.stack.extend(completed_items[::-1])\n",
    "            return\n",
    "        new_tree = ntree.Tree(head[0], [x[0] for x in completed_items[::-1]])\n",
    "        self.stack.append((new_tree, True))\n",
    "\n",
    "    def nonterminal(self, nont):\n",
    "        # put open non-terminal on stack\n",
    "        self.stack.append((nont, False))\n",
    "\n",
    "    def return_parse(self, force=False):\n",
    "        if not force:\n",
    "            assert self.done()\n",
    "            return self.stack[0][0]\n",
    "        assert force\n",
    "        # force cast to a tree\n",
    "        if len(self.stack) == 0:\n",
    "            candidate_tree = \"S\"\n",
    "        else:\n",
    "            candidate_tree = self.stack[-1][0]\n",
    "            candidate_trees = list(\n",
    "                filter(lambda x: isinstance(x[0], ntree.Tree), self.stack)\n",
    "            )\n",
    "            if len(candidate_trees) > 0:\n",
    "                candidate_tree = sorted(candidate_trees, key=lambda x: x[0].height())[\n",
    "                    -1\n",
    "                ][0]\n",
    "        assert isinstance(candidate_tree, ntree.Tree) or isinstance(\n",
    "            candidate_tree, str\n",
    "        ), candidate_tree\n",
    "        if not isinstance(candidate_tree, ntree.Tree):\n",
    "            candidate_tree = ntree.Tree(candidate_tree, [])\n",
    "        return candidate_tree\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.reduce_kw:\n",
    "            self.reduce()\n",
    "        elif action == self.shift_kw:\n",
    "            self.shift()\n",
    "        elif action == self.noop_kw:\n",
    "            pass\n",
    "        else:\n",
    "            self.nonterminal(action)\n",
    "\n",
    "    def done(self):\n",
    "        buffer_empty = len(self.buffer) == 0\n",
    "        single_item = len(self.stack) == 1\n",
    "        completed = False\n",
    "        if single_item:\n",
    "            completed = self.stack[0][1]\n",
    "        return buffer_empty and single_item and completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de2ba2f-a525-46e4-a65a-1e70f284b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell D\n",
    "# check validity of parser\n",
    "test_parser = ShiftReduceParser()\n",
    "for split, d in data.items():\n",
    "    for sentence, actions, tree in d:\n",
    "        test_parser = ShiftReduceParser()\n",
    "        test_parser.initialize_with_sentence(sentence)\n",
    "        for action in actions:\n",
    "            test_parser.step(action)\n",
    "        assert test_parser.done()\n",
    "        assert test_parser.return_parse() == tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc470a0d-5c41-4bbd-a6a6-fc51a4b5949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell E\n",
    "# trainer class\n",
    "# Q1: What do the \"dependency\" metrics measure?\n",
    "# Q1: You do not need to explain what precision, recall and F1 are,\n",
    "# Q1: but you do need to explain what sets they are computed against.\n",
    "\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.exact_match = None\n",
    "        self.edit_distance = None\n",
    "        self.precision = None\n",
    "        self.recall = None\n",
    "        self.f1 = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.exact_match = []\n",
    "        self.edit_distance = []\n",
    "        self.subtree_precision = []\n",
    "        self.subtree_recall = []\n",
    "        self.subtree_f1 = []\n",
    "        self.dependency_precision = []\n",
    "        self.dependency_recall = []\n",
    "        self.dependency_f1 = []\n",
    "\n",
    "    def overlap(self, forest1, forest2):\n",
    "        counter = 0\n",
    "        # candidate in forest1 can only match once\n",
    "        for t1 in forest1:\n",
    "            for t2 in forest2:\n",
    "                if t1 == t2:\n",
    "                    counter += 1\n",
    "                    break\n",
    "        return counter\n",
    "\n",
    "    def levenshtein_distance(self, a, b):\n",
    "        output = {}\n",
    "        assert isinstance(a, list)\n",
    "        assert isinstance(b, list)\n",
    "        for ii in range(1 + len(a)):\n",
    "            output[(ii, 0)] = ii\n",
    "            for jj in range(1 + len(b)):\n",
    "                output[(0, jj)] = jj\n",
    "                if ii and jj:\n",
    "                    output[(ii, jj)] = min(\n",
    "                        output[(ii - 1, jj)] + 1,\n",
    "                        output[(ii, jj - 1)] + 1,\n",
    "                        output[(ii - 1, jj - 1)] + int(a[ii - 1] != b[jj - 1]),\n",
    "                    )\n",
    "        return output[(len(a), len(b))]\n",
    "\n",
    "    def record_sequences(self, predictions, gold_sequences):\n",
    "        assert len(predictions) == len(gold_sequences)\n",
    "\n",
    "        def is_list_of_list_of_str(x):\n",
    "            output = isinstance(x, list)\n",
    "            for y in x:\n",
    "                output &= isinstance(y, list)\n",
    "                for z in y:\n",
    "                    output &= isinstance(z, str)\n",
    "            return output\n",
    "\n",
    "        assert is_list_of_list_of_str(gold_sequences), gold_sequences\n",
    "        assert is_list_of_list_of_str(predictions), predictions\n",
    "        for p, g in zip(predictions, gold_sequences):\n",
    "            self.edit_distance.append(self.levenshtein_distance(p, g) / len(g))\n",
    "\n",
    "    def all_links(self, tree):\n",
    "        assert isinstance(tree, str) or isinstance(tree, ntree.Tree)\n",
    "        if isinstance(tree, str):\n",
    "            return []\n",
    "        elif len(tree) == 0:\n",
    "            return []\n",
    "        else:\n",
    "            output = [\n",
    "                (tree.label(), child if isinstance(child, str) else child.label())\n",
    "                for child in tree\n",
    "            ]\n",
    "            for child in tree:\n",
    "                output.extend(self.all_links(child))\n",
    "            return output\n",
    "\n",
    "    def record_trees(self, predictions, gold_references):\n",
    "        assert len(predictions) == len(gold_references)\n",
    "        assert all(isinstance(g, ntree.Tree) for g in gold_references)\n",
    "        assert all(isinstance(p, ntree.Tree) for p in predictions), predictions\n",
    "        for p, g in zip(predictions, gold_references):\n",
    "            self.exact_match.append(int(p == g))\n",
    "            # dependencies\n",
    "            ps = self.all_links(p)\n",
    "            gs = self.all_links(g)\n",
    "            assert gs\n",
    "            ncp = self.overlap(ps, gs)  # number correct precision\n",
    "            ncr = self.overlap(gs, ps)  # number correct recall\n",
    "            lps = max(1, len(ps))\n",
    "            lgs = max(1, len(gs))\n",
    "            self.dependency_precision.append(ncp / lps)\n",
    "            self.dependency_recall.append(ncr / lgs)\n",
    "            f1 = 0\n",
    "            if ncr or ncp:\n",
    "                f1 = 2 * ncp * ncr / (lps * ncr + lgs * ncp)\n",
    "            self.dependency_f1.append(f1)\n",
    "            # subtrees\n",
    "            ps = list(p.subtrees())\n",
    "            gs = list(g.subtrees())\n",
    "            assert ps\n",
    "            assert gs\n",
    "            ncp = self.overlap(ps, gs)  # number correct precision\n",
    "            ncr = self.overlap(gs, ps)  # number correct recall\n",
    "            lps = max(1, len(ps))\n",
    "            lgs = max(1, len(gs))\n",
    "            self.subtree_precision.append(ncp / lps)\n",
    "            self.subtree_recall.append(ncr / lgs)\n",
    "            f1 = 0\n",
    "            if ncr or ncp:\n",
    "                f1 = 2 * ncp * ncr / (lps * ncr + lgs * ncp)\n",
    "            self.subtree_f1.append(f1)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        def mean(l):\n",
    "            return sum(l) / len(l)\n",
    "\n",
    "        output = {\n",
    "            \"exact match\": mean(self.exact_match),\n",
    "            \"dependency precision\": mean(self.dependency_precision),\n",
    "            \"dependency recall\": mean(self.dependency_recall),\n",
    "            \"dependency f1\": mean(self.dependency_f1),\n",
    "            \"subtree precision\": mean(self.subtree_precision),\n",
    "            \"subtree recall\": mean(self.subtree_recall),\n",
    "            \"subtree f1\": mean(self.subtree_f1),\n",
    "            \"edit distance\": mean(self.edit_distance),\n",
    "            \"N\": {\n",
    "                \"exact match\": len(self.exact_match),\n",
    "                \"dependency precision\": len(self.dependency_precision),\n",
    "                \"dependency recall\": len(self.dependency_recall),\n",
    "                \"dependency f1\": len(self.dependency_f1),\n",
    "                \"subtree precision\": len(self.subtree_precision),\n",
    "                \"subtree recall\": len(self.subtree_recall),\n",
    "                \"subtree f1\": len(self.subtree_f1),\n",
    "                \"edit distance\": len(self.edit_distance),\n",
    "            },\n",
    "        }\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd0001",
   "metadata": {},
   "source": [
    "**Cell E Q1: What do the \"dependency\" metrics measure? You do not need to explain what precision, recall and F1 are, but you do need to explain what sets they are computed against.**\n",
    "\n",
    "The dependency metrics evaluate how closely the predicted treeâ€™s dependency links align with those in the gold (reference) tree.\n",
    "\n",
    "Dependency precision measures the proportion of links in the predicted set that also exist in the gold set, while dependency recall quantifies the proportion of gold set links that are correctly identified in the predicted set. The F1 score is calculated as the harmonic mean of precision and recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3f8ead5-3a39-4b82-aa77-3deac827a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell F\n",
    "# BM25 retriever\n",
    "# Q2: What is BM25? How does this retriever work?\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "class FewShotRetriever:\n",
    "    def __init__(self, data_set):\n",
    "        # data_set contains items from the training set\n",
    "        assert isinstance(data_set, list)\n",
    "        assert all(isinstance(x, tuple) for x in data_set)\n",
    "        self.data = [(x[0], x[1]) for x in data_set]\n",
    "        self.bm25 = None\n",
    "\n",
    "    def build_index(self):\n",
    "        self.corpus = [x[0] for x in self.data]\n",
    "        self.bm25 = BM25Okapi(self.corpus)\n",
    "\n",
    "    def get_samples(self, query, n=4):\n",
    "        tokenized_query = query.split(\" \")\n",
    "        top_n = self.bm25.get_top_n(tokenized_query, self.data, n=n)\n",
    "        return top_n\n",
    "\n",
    "\n",
    "# build retriever on training data\n",
    "retriever = FewShotRetriever(data[\"train\"])\n",
    "retriever.build_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fcf85a",
   "metadata": {},
   "source": [
    "**Cell F Q2: What is BM25? How does this retriever work?**\n",
    "\n",
    "BM25 (Best Matching 25) is a ranking function used in information retrieval to score documents based on their relevance to a given query.\n",
    "The BM25 algorithm is an improved version of the TF-IDF algorithm. It uses TF (Term Frequency), IDF (Inverse Document Frequency) and Document Length Normalization to calculate the relevance of a document to a query.\n",
    "\n",
    "The FewShotRetriever class is designed to find the most relevant examples from a dataset using BM25.\n",
    "\n",
    "With the `build_index` method, the retriever creates the corpus: a collection of all the inputs of the dataset. This corpus will be then given to the BM25 algorithm to build the index.\n",
    "\n",
    "The `get_samples` method is used to find the most relevant examples from the dataset given a query. It returns the top_n examples with the highest BM25 scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61d6167-5e8b-499e-8348-d3e7bd02c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell G\n",
    "import numpy as np\n",
    "import scipy.stats as sh\n",
    "\n",
    "\n",
    "def valid_estimate(records):\n",
    "    assert isinstance(records, list)\n",
    "    assert len(records) > 0\n",
    "    assert all(isinstance(x, float) for x in records)\n",
    "    mu = np.mean(records)\n",
    "    interval = sh.t.interval(\n",
    "        confidence=0.9, df=len(records) - 1, loc=mu, scale=sh.sem(records)\n",
    "    )\n",
    "    criterion = (interval[1] - interval[0]) < 0.1\n",
    "\n",
    "    if criterion:\n",
    "        print(\n",
    "            \"mean: {:.3f}, interval: [{:.3f}, {:.3f}]\".format(\n",
    "                mu, interval[0], interval[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37c242ba-843b-4a40-b53a-5eecc5df418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell H\n",
    "# LLM parser\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import json\n",
    "\n",
    "\n",
    "class LLMParser:\n",
    "    def __init__(self, model_name, retriever, n_few_shot=2, max_tokens=100):\n",
    "        assert isinstance(retriever, FewShotRetriever)\n",
    "        self.retriever = retriever\n",
    "        assert isinstance(max_tokens, int)\n",
    "        self.max_tokens = max_tokens\n",
    "        assert isinstance(model_name, str)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        assert isinstance(n_few_shot, int)\n",
    "        self.n_few_shot = n_few_shot\n",
    "\n",
    "    def compute_flops(self, input_text):\n",
    "        \"\"\"Profiles the number of FLOPs required for a single forward pass.\"\"\"\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        with torch.profiler.profile(\n",
    "            activities=[\n",
    "                torch.profiler.ProfilerActivity.CPU,\n",
    "                torch.profiler.ProfilerActivity.CUDA,\n",
    "            ],\n",
    "            record_shapes=True,\n",
    "            profile_memory=True,\n",
    "            with_flops=True,\n",
    "        ) as prof:\n",
    "            with torch.no_grad():\n",
    "                self.model(**inputs)\n",
    "\n",
    "        total_flops = sum(\n",
    "            evt.flops for evt in prof.key_averages() if evt.flops is not None\n",
    "        )\n",
    "        return total_flops\n",
    "\n",
    "    def generate(self, input_strings):\n",
    "        assert isinstance(input_strings, list)\n",
    "        assert len(input_strings) > 0\n",
    "        assert all(isinstance(ii, str) for ii in input_strings)\n",
    "\n",
    "        prompts = self.make_prompts(input_strings)\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        )\n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": self.max_tokens,\n",
    "            \"return_full_text\": False,\n",
    "            \"do_sample\": False,\n",
    "            \"stop_strings\": [\"\\n\"],\n",
    "            \"tokenizer\": self.tokenizer,\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "        }\n",
    "\n",
    "        # for each prompt calculate the flops and sum them\n",
    "        total_flops = 0\n",
    "        output = []\n",
    "        for prompt in prompts:\n",
    "            total_flops += self.compute_flops(prompt)\n",
    "\n",
    "            generated_text = pipe(prompt, **generation_args)[0][\"generated_text\"]\n",
    "            output.append({\"prompt\": prompt, \"lm_output\": generated_text})\n",
    "\n",
    "        # average flops and token length per prompt\n",
    "        avg_prompt_flops = total_flops / len(prompts)\n",
    "        avg_prompt_token_length = sum(\n",
    "            len(self.tokenizer(x).input_ids) for x in prompts\n",
    "        ) / len(prompts)\n",
    "        return output, avg_prompt_flops, avg_prompt_token_length\n",
    "\n",
    "    def make_prompt(self, input_string):\n",
    "        samples = self.retriever.get_samples(input_string, n=self.n_few_shot)\n",
    "        # assert len(samples) == self.n_few_shot\n",
    "        prompt = [\"{}\\n{}\".format(\" \".join(x), \" \".join(y)) for x, y in samples]\n",
    "        prompt.append(\"{}\\n\".format(input_string))\n",
    "        prompt = \"\\n\".join(prompt)\n",
    "        return prompt\n",
    "\n",
    "    def make_prompts(self, input_strings):\n",
    "        prompts = [self.make_prompt(input_string) for input_string in input_strings]\n",
    "        return prompts\n",
    "\n",
    "    def eval(self, data, batch_size=16, variable_size=False):\n",
    "        if variable_size:\n",
    "            return self.eval_variable_size(data, batch_size=batch_size)\n",
    "        metrics = Metrics()\n",
    "        metrics.reset()\n",
    "        total_flops = 0\n",
    "        total_prompt_token_length = 0\n",
    "        num_batches = len(data) // batch_size\n",
    "        for position in tqdm(range(0, len(data), batch_size)):\n",
    "            # chunk is a list of tuples (input_seq, action_seq, tree)\n",
    "            chunk = data[position : position + batch_size]\n",
    "            assert len(chunk) > 0\n",
    "\n",
    "            # gets the flops and prompt length for the batch (chunk)\n",
    "            avg_flops, avg_prompt_token_length = self.eval_step(chunk, metrics)\n",
    "            total_flops += avg_flops\n",
    "            total_prompt_token_length += avg_prompt_token_length\n",
    "\n",
    "        return (\n",
    "            metrics.get_metrics(),\n",
    "            total_flops / num_batches,\n",
    "            total_prompt_token_length / num_batches,\n",
    "        )\n",
    "\n",
    "    def eval_variable_size(self, data, batch_size=16):\n",
    "        metrics = Metrics()\n",
    "        metrics.reset()\n",
    "        position = 0\n",
    "        terminate = False\n",
    "        total_flops = 0\n",
    "        total_prompt_token_length = 0\n",
    "\n",
    "        while (position < len(data)) and (not terminate):\n",
    "            # chunk is a list of tuples (input_seq, action_seq, tree)\n",
    "            chunk = data[position : position + batch_size]\n",
    "            assert len(chunk) > 0\n",
    "\n",
    "            # gets the flops and prompt length for the batch (chunk)\n",
    "            avg_flops, avg_prompt_token_length = self.eval_step(chunk, metrics)\n",
    "            total_flops += avg_flops\n",
    "            total_prompt_token_length += avg_prompt_token_length\n",
    "\n",
    "            position += batch_size\n",
    "            terminate = valid_estimate(metrics.edit_distance)\n",
    "\n",
    "        num_batches = position / batch_size\n",
    "        return (\n",
    "            metrics.get_metrics(),\n",
    "            total_flops / num_batches,\n",
    "            total_prompt_token_length / num_batches,\n",
    "        )\n",
    "\n",
    "    def eval_step(self, chunk, metrics):\n",
    "        # chunk is a list of tuples (input_seq, action_seq, tree)\n",
    "        sentences = [\n",
    "            \" \".join(x[0]) for x in chunk\n",
    "        ]  # this is a very basic form of detokenization\n",
    "        y, avg_flops, avg_prompt_token_length = self.generate(sentences)\n",
    "        generated_sequences = [x[\"lm_output\"].split() for x in y]\n",
    "        metrics.record_sequences(generated_sequences, [x[1] for x in chunk])\n",
    "        trees = []\n",
    "        for ii, sentence in enumerate(sentences):\n",
    "            parser = ShiftReduceParser()\n",
    "            parser.initialize_with_sentence(sentence.split())\n",
    "            for action in generated_sequences[ii]:\n",
    "                parser.step(action)\n",
    "            trees.append(parser.return_parse(force=True))\n",
    "        assert len(trees) == len(chunk)\n",
    "        metrics.record_trees(trees, [x[2] for x in chunk])\n",
    "\n",
    "        # returns the average number of flops per sentence and the average prompt length,\n",
    "        # calculated over the batch (chunk)\n",
    "        return avg_flops, avg_prompt_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22357cda-b2a0-409c-a1a9-dc37314dbef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Model: Qwen/Qwen2.5-1.5B ===\n",
      "\n",
      "=== 0-Shot ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.997, interval: [0.995, 0.999]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.0,\n",
      "    \"dependency recall\": 0.0,\n",
      "    \"dependency f1\": 0.0,\n",
      "    \"subtree precision\": 0.0,\n",
      "    \"subtree recall\": 0.0,\n",
      "    \"subtree f1\": 0.0,\n",
      "    \"edit distance\": 0.9970198101954588,\n",
      "    \"N\": {\n",
      "      \"exact match\": 16,\n",
      "      \"dependency precision\": 16,\n",
      "      \"dependency recall\": 16,\n",
      "      \"dependency f1\": 16,\n",
      "      \"subtree precision\": 16,\n",
      "      \"subtree recall\": 16,\n",
      "      \"subtree f1\": 16,\n",
      "      \"edit distance\": 16\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 94.94730021675,\n",
      "  \"average prompt length (tokens) (with 0-shot)\": 30.75\n",
      "}\n",
      "\n",
      "=== 1-Shot ===\n",
      "mean: 0.847, interval: [0.801, 0.893]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.16867933223398582,\n",
      "    \"dependency recall\": 0.0518067233423056,\n",
      "    \"dependency f1\": 0.07133723491715335,\n",
      "    \"subtree precision\": 0.03918931640971115,\n",
      "    \"subtree recall\": 0.008796309940568453,\n",
      "    \"subtree f1\": 0.012932719250520308,\n",
      "    \"edit distance\": 0.8467094026410223,\n",
      "    \"N\": {\n",
      "      \"exact match\": 48,\n",
      "      \"dependency precision\": 48,\n",
      "      \"dependency recall\": 48,\n",
      "      \"dependency f1\": 48,\n",
      "      \"subtree precision\": 48,\n",
      "      \"subtree recall\": 48,\n",
      "      \"subtree f1\": 48,\n",
      "      \"edit distance\": 48\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 856.7764598296459,\n",
      "  \"average prompt length (tokens) (with 1-shot)\": 277.4791666666667\n",
      "}\n",
      "\n",
      "=== 2-Shot ===\n",
      "mean: 0.600, interval: [0.559, 0.641]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.2971291624711239,\n",
      "    \"dependency recall\": 0.11873838726065943,\n",
      "    \"dependency f1\": 0.15363053231181806,\n",
      "    \"subtree precision\": 0.07850511416687887,\n",
      "    \"subtree recall\": 0.027974991791562584,\n",
      "    \"subtree f1\": 0.03920203283838941,\n",
      "    \"edit distance\": 0.5997944390673585,\n",
      "    \"N\": {\n",
      "      \"exact match\": 16,\n",
      "      \"dependency precision\": 16,\n",
      "      \"dependency recall\": 16,\n",
      "      \"dependency f1\": 16,\n",
      "      \"subtree precision\": 16,\n",
      "      \"subtree recall\": 16,\n",
      "      \"subtree f1\": 16,\n",
      "      \"edit distance\": 16\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 1662.1552556038125,\n",
      "  \"average prompt length (tokens) (with 2-shot)\": 538.3125\n",
      "}\n",
      "\n",
      "=== 4-Shot ===\n",
      "mean: 0.609, interval: [0.563, 0.655]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.3164001101452727,\n",
      "    \"dependency recall\": 0.0997861711112315,\n",
      "    \"dependency f1\": 0.13389608312951623,\n",
      "    \"subtree precision\": 0.08466880341880344,\n",
      "    \"subtree recall\": 0.019100776375060316,\n",
      "    \"subtree f1\": 0.028730400224516617,\n",
      "    \"edit distance\": 0.6091924490720939,\n",
      "    \"N\": {\n",
      "      \"exact match\": 16,\n",
      "      \"dependency precision\": 16,\n",
      "      \"dependency recall\": 16,\n",
      "      \"dependency f1\": 16,\n",
      "      \"subtree precision\": 16,\n",
      "      \"subtree recall\": 16,\n",
      "      \"subtree f1\": 16,\n",
      "      \"edit distance\": 16\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 3040.240714219625,\n",
      "  \"average prompt length (tokens) (with 4-shot)\": 984.625\n",
      "}\n",
      "\n",
      "=== 8-Shot ===\n",
      "mean: 0.591, interval: [0.553, 0.629]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.27792085171319436,\n",
      "    \"dependency recall\": 0.09854075266463772,\n",
      "    \"dependency f1\": 0.1321703467329848,\n",
      "    \"subtree precision\": 0.017586580086580088,\n",
      "    \"subtree recall\": 0.004705152013956302,\n",
      "    \"subtree f1\": 0.007227564102564104,\n",
      "    \"edit distance\": 0.5908578445225565,\n",
      "    \"N\": {\n",
      "      \"exact match\": 16,\n",
      "      \"dependency precision\": 16,\n",
      "      \"dependency recall\": 16,\n",
      "      \"dependency f1\": 16,\n",
      "      \"subtree precision\": 16,\n",
      "      \"subtree recall\": 16,\n",
      "      \"subtree f1\": 16,\n",
      "      \"edit distance\": 16\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 6027.797216786687,\n",
      "  \"average prompt length (tokens) (with 8-shot)\": 1952.1875\n",
      "}\n",
      "\n",
      "=== 16-Shot ===\n",
      "mean: 0.608, interval: [0.571, 0.644]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.3261215777387215,\n",
      "    \"dependency recall\": 0.09935890613151951,\n",
      "    \"dependency f1\": 0.14282731429885198,\n",
      "    \"subtree precision\": 0.024258378623188408,\n",
      "    \"subtree recall\": 0.014442389076504821,\n",
      "    \"subtree f1\": 0.015825264072550194,\n",
      "    \"edit distance\": 0.6078411983547212,\n",
      "    \"N\": {\n",
      "      \"exact match\": 32,\n",
      "      \"dependency precision\": 32,\n",
      "      \"dependency recall\": 32,\n",
      "      \"dependency f1\": 32,\n",
      "      \"subtree precision\": 32,\n",
      "      \"subtree recall\": 32,\n",
      "      \"subtree f1\": 32,\n",
      "      \"edit distance\": 32\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 12457.190178450937,\n",
      "  \"average prompt length (tokens) (with 16-shot)\": 4034.4375\n",
      "}\n",
      "\n",
      "=== Evaluating Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B ===\n",
      "\n",
      "=== 0-Shot ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ids/glorenzo-23/IA327-Generative-Models-for-NLP/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ids/glorenzo-23/IA327-Generative-Models-for-NLP/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 1.015, interval: [0.986, 1.043]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.0,\n",
      "    \"dependency recall\": 0.0,\n",
      "    \"dependency f1\": 0.0,\n",
      "    \"subtree precision\": 0.0,\n",
      "    \"subtree recall\": 0.0,\n",
      "    \"subtree f1\": 0.0,\n",
      "    \"edit distance\": 1.0146015259473553,\n",
      "    \"N\": {\n",
      "      \"exact match\": 16,\n",
      "      \"dependency precision\": 16,\n",
      "      \"dependency recall\": 16,\n",
      "      \"dependency f1\": 16,\n",
      "      \"subtree precision\": 16,\n",
      "      \"subtree recall\": 16,\n",
      "      \"subtree f1\": 16,\n",
      "      \"edit distance\": 16\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 98.03501444975,\n",
      "  \"average prompt length (tokens) (with 0-shot)\": 31.75\n",
      "}\n",
      "\n",
      "=== 1-Shot ===\n",
      "mean: 0.950, interval: [0.901, 0.998]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.0625,\n",
      "    \"dependency recall\": 0.02219970448014629,\n",
      "    \"dependency f1\": 0.0316643876512843,\n",
      "    \"subtree precision\": 0.011363636363636364,\n",
      "    \"subtree recall\": 0.003205128205128205,\n",
      "    \"subtree f1\": 0.005,\n",
      "    \"edit distance\": 0.9496851557096684,\n",
      "    \"N\": {\n",
      "      \"exact match\": 32,\n",
      "      \"dependency precision\": 32,\n",
      "      \"dependency recall\": 32,\n",
      "      \"dependency f1\": 32,\n",
      "      \"subtree precision\": 32,\n",
      "      \"subtree recall\": 32,\n",
      "      \"subtree f1\": 32,\n",
      "      \"edit distance\": 32\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 837.156608974125,\n",
      "  \"average prompt length (tokens) (with 1-shot)\": 271.125\n",
      "}\n",
      "\n",
      "=== 2-Shot ===\n",
      "mean: 0.800, interval: [0.752, 0.848]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.17317249743720334,\n",
      "    \"dependency recall\": 0.0338351610126406,\n",
      "    \"dependency f1\": 0.051333229904585144,\n",
      "    \"subtree precision\": 0.0189484126984127,\n",
      "    \"subtree recall\": 0.002820007869144129,\n",
      "    \"subtree f1\": 0.004728186013853265,\n",
      "    \"edit distance\": 0.8001287661653144,\n",
      "    \"N\": {\n",
      "      \"exact match\": 48,\n",
      "      \"dependency precision\": 48,\n",
      "      \"dependency recall\": 48,\n",
      "      \"dependency f1\": 48,\n",
      "      \"subtree precision\": 48,\n",
      "      \"subtree recall\": 48,\n",
      "      \"subtree f1\": 48,\n",
      "      \"edit distance\": 48\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 1617.962345644,\n",
      "  \"average prompt length (tokens) (with 2-shot)\": 524.0\n",
      "}\n",
      "\n",
      "=== 4-Shot ===\n",
      "mean: 0.618, interval: [0.581, 0.655]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.29679998820623815,\n",
      "    \"dependency recall\": 0.08380508038984762,\n",
      "    \"dependency f1\": 0.11588173344684487,\n",
      "    \"subtree precision\": 0.06856060606060606,\n",
      "    \"subtree recall\": 0.015234786246121769,\n",
      "    \"subtree f1\": 0.02214162387457857,\n",
      "    \"edit distance\": 0.6178705741817019,\n",
      "    \"N\": {\n",
      "      \"exact match\": 16,\n",
      "      \"dependency precision\": 16,\n",
      "      \"dependency recall\": 16,\n",
      "      \"dependency f1\": 16,\n",
      "      \"subtree precision\": 16,\n",
      "      \"subtree recall\": 16,\n",
      "      \"subtree f1\": 16,\n",
      "      \"edit distance\": 16\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 3043.328428452625,\n",
      "  \"average prompt length (tokens) (with 4-shot)\": 985.625\n",
      "}\n",
      "\n",
      "=== 8-Shot ===\n",
      "mean: 0.631, interval: [0.587, 0.674]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.34851227421400743,\n",
      "    \"dependency recall\": 0.100980481751475,\n",
      "    \"dependency f1\": 0.1362055062706615,\n",
      "    \"subtree precision\": 0.07613910847606499,\n",
      "    \"subtree recall\": 0.01862742287685637,\n",
      "    \"subtree f1\": 0.024065524548216647,\n",
      "    \"edit distance\": 0.6305820555088059,\n",
      "    \"N\": {\n",
      "      \"exact match\": 48,\n",
      "      \"dependency precision\": 48,\n",
      "      \"dependency recall\": 48,\n",
      "      \"dependency f1\": 48,\n",
      "      \"subtree precision\": 48,\n",
      "      \"subtree recall\": 48,\n",
      "      \"subtree f1\": 48,\n",
      "      \"edit distance\": 48\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 6096.6275132306455,\n",
      "  \"average prompt length (tokens) (with 8-shot)\": 1974.4791666666667\n",
      "}\n",
      "\n",
      "=== 16-Shot ===\n",
      "mean: 0.604, interval: [0.565, 0.643]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.35231419009646003,\n",
      "    \"dependency recall\": 0.09748001276103181,\n",
      "    \"dependency f1\": 0.1319127091328396,\n",
      "    \"subtree precision\": 0.07020930002015528,\n",
      "    \"subtree recall\": 0.018781133922159295,\n",
      "    \"subtree f1\": 0.02644255005873105,\n",
      "    \"edit distance\": 0.6043394494038421,\n",
      "    \"N\": {\n",
      "      \"exact match\": 32,\n",
      "      \"dependency precision\": 32,\n",
      "      \"dependency recall\": 32,\n",
      "      \"dependency f1\": 32,\n",
      "      \"subtree precision\": 32,\n",
      "      \"subtree recall\": 32,\n",
      "      \"subtree f1\": 32,\n",
      "      \"edit distance\": 32\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 12460.277892683938,\n",
      "  \"average prompt length (tokens) (with 16-shot)\": 4035.4375\n",
      "}\n",
      "\n",
      "=== Evaluating Model: HuggingFaceTB/SmolLM-135M ===\n",
      "\n",
      "=== 0-Shot ===\n",
      "mean: 1.002, interval: [0.999, 1.005]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.0,\n",
      "    \"dependency recall\": 0.0,\n",
      "    \"dependency f1\": 0.0,\n",
      "    \"subtree precision\": 0.0,\n",
      "    \"subtree recall\": 0.0,\n",
      "    \"subtree f1\": 0.0,\n",
      "    \"edit distance\": 1.0016666666666667,\n",
      "    \"N\": {\n",
      "      \"exact match\": 16,\n",
      "      \"dependency precision\": 16,\n",
      "      \"dependency recall\": 16,\n",
      "      \"dependency f1\": 16,\n",
      "      \"subtree precision\": 16,\n",
      "      \"subtree recall\": 16,\n",
      "      \"subtree f1\": 16,\n",
      "      \"edit distance\": 16\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 8.67995793525,\n",
      "  \"average prompt length (tokens) (with 0-shot)\": 32.25\n",
      "}\n",
      "\n",
      "=== 1-Shot ===\n",
      "mean: 0.919, interval: [0.875, 0.964]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.07490808823529412,\n",
      "    \"dependency recall\": 0.014710840091077343,\n",
      "    \"dependency f1\": 0.021379513343799058,\n",
      "    \"subtree precision\": 0.010416666666666666,\n",
      "    \"subtree recall\": 0.00040584415584415587,\n",
      "    \"subtree f1\": 0.00078125,\n",
      "    \"edit distance\": 0.9192988395272217,\n",
      "    \"N\": {\n",
      "      \"exact match\": 32,\n",
      "      \"dependency precision\": 32,\n",
      "      \"dependency recall\": 32,\n",
      "      \"dependency f1\": 32,\n",
      "      \"subtree precision\": 32,\n",
      "      \"subtree recall\": 32,\n",
      "      \"subtree f1\": 32,\n",
      "      \"edit distance\": 32\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 96.9426612834375,\n",
      "  \"average prompt length (tokens) (with 1-shot)\": 360.1875\n",
      "}\n",
      "\n",
      "=== 2-Shot ===\n",
      "mean: 0.712, interval: [0.675, 0.750]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.23070029157611394,\n",
      "    \"dependency recall\": 0.06892484022421222,\n",
      "    \"dependency f1\": 0.0984171064370549,\n",
      "    \"subtree precision\": 0.03824664918414919,\n",
      "    \"subtree recall\": 0.010078376521085949,\n",
      "    \"subtree f1\": 0.014587674452413256,\n",
      "    \"edit distance\": 0.7124909540364084,\n",
      "    \"N\": {\n",
      "      \"exact match\": 32,\n",
      "      \"dependency precision\": 32,\n",
      "      \"dependency recall\": 32,\n",
      "      \"dependency f1\": 32,\n",
      "      \"subtree precision\": 32,\n",
      "      \"subtree recall\": 32,\n",
      "      \"subtree f1\": 32,\n",
      "      \"edit distance\": 32\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 192.32929351821875,\n",
      "  \"average prompt length (tokens) (with 2-shot)\": 714.59375\n",
      "}\n",
      "\n",
      "=== 4-Shot ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.708, interval: [0.670, 0.747]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.279573019439169,\n",
      "    \"dependency recall\": 0.08837495815813207,\n",
      "    \"dependency f1\": 0.12349594296314563,\n",
      "    \"subtree precision\": 0.07589982015533486,\n",
      "    \"subtree recall\": 0.0165953116287721,\n",
      "    \"subtree f1\": 0.02536222855379309,\n",
      "    \"edit distance\": 0.7084253128834364,\n",
      "    \"N\": {\n",
      "      \"exact match\": 32,\n",
      "      \"dependency precision\": 32,\n",
      "      \"dependency recall\": 32,\n",
      "      \"dependency f1\": 32,\n",
      "      \"subtree precision\": 32,\n",
      "      \"subtree recall\": 32,\n",
      "      \"subtree f1\": 32,\n",
      "      \"edit distance\": 32\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 372.95915983284374,\n",
      "  \"average prompt length (tokens) (with 4-shot)\": 1385.71875\n",
      "}\n",
      "\n",
      "=== 8-Shot ===\n",
      "mean: 0.929, interval: [0.883, 0.975]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.07648189484126984,\n",
      "    \"dependency recall\": 0.027015923514517773,\n",
      "    \"dependency f1\": 0.03754813395243906,\n",
      "    \"subtree precision\": 0.012279040404040403,\n",
      "    \"subtree recall\": 0.003271901709401709,\n",
      "    \"subtree f1\": 0.005164399092970521,\n",
      "    \"edit distance\": 0.9286127407942294,\n",
      "    \"N\": {\n",
      "      \"exact match\": 32,\n",
      "      \"dependency precision\": 32,\n",
      "      \"dependency recall\": 32,\n",
      "      \"dependency f1\": 32,\n",
      "      \"subtree precision\": 32,\n",
      "      \"subtree recall\": 32,\n",
      "      \"subtree f1\": 32,\n",
      "      \"edit distance\": 32\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 739.3578777628124,\n",
      "  \"average prompt length (tokens) (with 8-shot)\": 2747.0625\n",
      "}\n",
      "\n",
      "=== 16-Shot ===\n",
      "mean: 0.993, interval: [0.988, 0.998]\n",
      "{\n",
      "  \"metrics\": {\n",
      "    \"exact match\": 0.0,\n",
      "    \"dependency precision\": 0.0,\n",
      "    \"dependency recall\": 0.0,\n",
      "    \"dependency f1\": 0.0,\n",
      "    \"subtree precision\": 0.0,\n",
      "    \"subtree recall\": 0.0,\n",
      "    \"subtree f1\": 0.0,\n",
      "    \"edit distance\": 0.993104588684408,\n",
      "    \"N\": {\n",
      "      \"exact match\": 16,\n",
      "      \"dependency precision\": 16,\n",
      "      \"dependency recall\": 16,\n",
      "      \"dependency f1\": 16,\n",
      "      \"subtree precision\": 16,\n",
      "      \"subtree recall\": 16,\n",
      "      \"subtree f1\": 16,\n",
      "      \"edit distance\": 16\n",
      "    }\n",
      "  },\n",
      "  \"flops per prompt (GFLOPs)\": 1449.41257016425,\n",
      "  \"average prompt length (tokens) (with 16-shot)\": 5385.25\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell I\n",
    "# TODO report 90% confidence interval. See function \"valid_estimate\" in cell G\n",
    "\n",
    "models = [\n",
    "    \"Qwen/Qwen2.5-1.5B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"HuggingFaceTB/SmolLM-135M\",\n",
    "]\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\n=== Evaluating Model: {model_name} ===\")\n",
    "\n",
    "    for n_few_shot in [0, 1, 2, 4, 8, 16]:\n",
    "        print(f\"\\n=== {n_few_shot}-Shot ===\")\n",
    "\n",
    "        llm_parser = LLMParser(model_name, retriever)\n",
    "        llm_parser.n_few_shot = n_few_shot\n",
    "        # gets the metrics, the average flops performed per prompt, and the average prompt length in tokens\n",
    "        metrics, avg_flops_per_prompt, avg_prompt_length = llm_parser.eval(\n",
    "            data[\"test\"], variable_size=True\n",
    "        )\n",
    "\n",
    "        results = {\n",
    "            \"metrics\": metrics,\n",
    "            \"flops per prompt (GFLOPs)\": avg_flops_per_prompt / 1e9,\n",
    "            f\"average prompt length (tokens) (with {n_few_shot}-shot)\": avg_prompt_length,\n",
    "        }\n",
    "\n",
    "        print(json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
